{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91f60f30-7986-464c-b65d-3813eba1af3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_14896\\341252622.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n",
      "C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_14896\\341252622.py:126: DtypeWarning: Columns (9,10,13,28,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  targets_harvey = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/targets/disaster_4332.csv')\n",
      "C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_14896\\341252622.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'labels': torch.tensor(label, dtype=torch.float32)\n",
      "Making predictions: 100%|█████████████████████████████████████████████████████████████| 9/9 [00:23<00:00,  2.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to Training_results/Harvey_5foldcv_BERT_Multihead/bert_metrics.xlsx\n",
      "Mean predictions saved to Training_results/Harvey_5foldcv_BERT_Multihead/bert_mean_predictions.xlsx\n",
      "Binary predictions saved to Training_results/Harvey_5foldcv_BERT_Multihead/bert_binary_predictions.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)\n",
    "\n",
    "# Load the saved models\n",
    "save_path = 'Training_results/Harvey_5foldcv_BERT_Multihead/'\n",
    "saved_model_paths = [os.path.join(save_path, f\"best_bert_model_fold_{i+1}.pth\") for i in range(5)]\n",
    "models = []\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the target list\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959  \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "# Get the target list if not provided\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n",
    "\n",
    "# Define dataset class to handle tokenization and data loading\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class BERTDeepMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, num_targets=11, hidden_dim=256):\n",
    "        super(BERTDeepMultiHeadClassifier, self).__init__()\n",
    "        self.bert = model_bert\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.drop(pooled_output)\n",
    "        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n",
    "\n",
    "# Load all trained models\n",
    "for path in saved_model_paths:\n",
    "    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "size_threshold = 80\n",
    "\n",
    "tweets_harvey2 = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/tweets/harvey_corrected.csv')\n",
    "tweets_harvey2.rename(columns={'zipcode': 'zip_code'}, inplace=True)\n",
    "\n",
    "bboxes_useful = tweets_harvey2.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_harvey = tweets_harvey2.loc[((tweets_harvey2.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "tweets_harvey.loc[:, 'zip_code'] = tweets_harvey['zip_code'].apply(int)\n",
    "tweet_grouped_harvey = tweets_harvey.groupby('zip_code')\n",
    "\n",
    "targets_harvey = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/targets/disaster_4332.csv')\n",
    "target_grouped_harvey = targets_harvey.groupby('damagedZipCode')\n",
    "    \n",
    "tweet_dict = {int(name): group['text'] for name, group in tweet_grouped_harvey}\n",
    "target_dict = {int(name): group[target_list] for name, group in target_grouped_harvey}\n",
    "\n",
    "intersecting_zips = list(set(target_dict.keys()) & set(tweet_dict.keys()))\n",
    "paired_data = {\n",
    "    name: [target_dict[name].sum().apply(lambda x: 1 if x > 0 else 0), tweet_dict[name]] for name in intersecting_zips\n",
    "}\n",
    "\n",
    "# Prepare dataset for inference\n",
    "texts = ['\\n'.join(v[1].to_list()) for v in paired_data.values()]\n",
    "labels_ = [v[0] for v in paired_data.values()]\n",
    "zip_codes = list(paired_data.keys())\n",
    "\n",
    "test_dataset = CustomDataset(texts, labels_, tokenizer, max_len=256)\n",
    "test_loader = DataLoader(test_dataset, batch_size=48, shuffle=False)\n",
    "\n",
    "# Store predictions\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Making predictions\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        fold_preds = []\n",
    "        for model in models:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            fold_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "# Convert list to numpy array\n",
    "all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n",
    "mean_preds = all_preds.mean(axis=0)  # Mean prediction probabilities (Shape: total_samples, num_targets)\n",
    "binary_preds = (mean_preds > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Convert labels to numpy array\n",
    "true_labels = np.vstack(labels_)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "metrics_dict = {'Target': target_list, 'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': []}\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    metrics_dict['Accuracy'].append(accuracy_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['F1_Score'].append(f1_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Precision'].append(precision_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Recall'].append(recall_score(true_labels[:, i], binary_preds[:, i]))\n",
    "\n",
    "# Compute and append average metrics\n",
    "metrics_dict['Target'].append('Average')\n",
    "metrics_dict['Accuracy'].append(np.mean(metrics_dict['Accuracy']))\n",
    "metrics_dict['F1_Score'].append(np.mean(metrics_dict['F1_Score']))\n",
    "metrics_dict['Precision'].append(np.mean(metrics_dict['Precision']))\n",
    "metrics_dict['Recall'].append(np.mean(metrics_dict['Recall']))\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "metrics_df_path = os.path.join(save_path, \"bert_metrics.xlsx\")\n",
    "metrics_df.to_excel(metrics_df_path, index=False)\n",
    "\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "# Save mean prediction probabilities\n",
    "mean_preds_df = pd.DataFrame(mean_preds, columns=target_list)\n",
    "mean_preds_df.insert(0, \"zip_code\", zip_codes)\n",
    "mean_preds_path = os.path.join(save_path, \"bert_mean_predictions.xlsx\")\n",
    "mean_preds_df.to_excel(mean_preds_path, index=False)\n",
    "\n",
    "# Save binary predictions\n",
    "binary_preds_df = pd.DataFrame(binary_preds, columns=target_list)\n",
    "binary_preds_df.insert(0, \"zip_code\", zip_codes)\n",
    "binary_preds_path = os.path.join(save_path, \"bert_binary_predictions.xlsx\")\n",
    "binary_preds_df.to_excel(binary_preds_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to {metrics_df_path}\")\n",
    "print(f\"Mean predictions saved to {mean_preds_path}\")\n",
    "print(f\"Binary predictions saved to {binary_preds_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250dee42-31ee-44ca-b31b-f3c98b9dc071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
