{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "Predict multiple disaster impact categories (e.g., damage levels, eligibility) based on tweet text and geospatial metadata using a feedforward neural network trained on grouped tweets and zip-code-level labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Use geospatially-filtered tweets (based on bounding box size) to train a feedforward model for multi-label binary classification, one model per target label, using zip-code grouped tweet aggregation and target data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import preprocessor\n",
    "import trainer\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "torch.manual_seed(64)\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haversine and Bounding Box Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959  # miles\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "    return R * (2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)))\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    return haversine(lat1, lon1, lat1, lon2) * haversine(lat1, lon1, lat2, lon1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplerNet(nn.Module):\n",
    "    def __init__(self, in_out_degrees, output_size, sigmoid=False):\n",
    "        super().__init__()\n",
    "        self.sigmoid = sigmoid\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(in_out_degrees) - 1):\n",
    "            self.layers.append(nn.Linear(in_out_degrees[i], in_out_degrees[i + 1]))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(in_out_degrees[i + 1]))\n",
    "\n",
    "        self.output_layer = nn.Linear(in_out_degrees[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for fc, bn in zip(self.layers, self.batch_norms):\n",
    "            x = torch.relu(bn(fc(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x) if self.sigmoid else x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2 = pd.read_csv('organized_with_zipcode.csv')\n",
    "tweets_harvey2 = pd.read_csv('harvey_corrected.csv')\n",
    "tweets_harvey2.rename(columns={'zipcode': 'zip_code'}, inplace=True)\n",
    "\n",
    "# Filter Harvey tweets\n",
    "size_threshold = 60\n",
    "bboxes_useful = tweets_harvey2.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1, 0, 3, 2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_harvey = tweets_harvey2.loc[((tweets_harvey2.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "tweets_harvey['zip_code'] = tweets_harvey['zip_code'].astype(int)\n",
    "\n",
    "# Filter other storms\n",
    "tweets = tweets2[tweets2.storm_name.isin(['imelda', 'beryl'])]\n",
    "bboxes_useful = tweets.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1, 0, 3, 2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets = tweets.loc[((tweets.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "\n",
    "tweet_grouped_everything = pd.concat([\n",
    "    tweets2.loc[:, tweets2.columns.intersection(tweets_harvey2.columns)],\n",
    "    tweets_harvey2.loc[:, tweets2.columns.intersection(tweets_harvey2.columns)]\n",
    "])\n",
    "bboxes_useful = tweet_grouped_everything.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1, 0, 3, 2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweet_grouped_everything = tweet_grouped_everything.loc[((tweet_grouped_everything.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "\n",
    "# Load targets\n",
    "targets_beryl = pd.read_csv('targets/disaster_4798.csv')\n",
    "targets_imelda = pd.read_csv('targets/disaster_4466.csv')\n",
    "targets_harvey = pd.read_csv('targets/disaster_4332.csv')\n",
    "targets_everything = pd.concat([targets_beryl, targets_imelda, targets_harvey])\n",
    "\n",
    "# Group tweets and targets\n",
    "tweet_grouped = tweet_grouped_everything.groupby('zip_code')\n",
    "target_grouped = targets_everything.groupby('damagedZipCode')\n",
    "\n",
    "# Preprocessing via external script\n",
    "train_dl, val_dl, class_weights = preprocessor.clean(tweet_grouped, target_grouped, val_batch_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train All Target Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = []\n",
    "metadatas = []\n",
    "in_out_degrees = [128, 32]\n",
    "num_epochs = 300\n",
    "\n",
    "for i in range(11):\n",
    "    model = SimplerNet(in_out_degrees, output_size=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "    criterion = nn.BCEWithLogitsLoss(weight=class_weights[i].to(device))\n",
    "    \n",
    "    model, history = trainer.single_target_loop(\n",
    "        model, optimizer, criterion, num_epochs,\n",
    "        train_dl, val_dl, which_target=i, device=device,\n",
    "        previous_loss_scale=1.007, epoch_percentage=0.1\n",
    "    )\n",
    "    \n",
    "    histories.append(history)\n",
    "    meta = f\"\"\"Loss: BCEWithLogits; Layers={len(in_out_degrees)}, BBox < {size_threshold} miÂ², Optimizer=Adam\"\"\"\n",
    "    metadatas.append(meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_labels = ['everything ' + i for i in preprocessor.get_target_list()]\n",
    "\n",
    "def make_canvas(num_plots):\n",
    "    nrows = math.ceil(num_plots / 4)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=4, figsize=(6 * 4, 6 * nrows))\n",
    "    return fig, axes\n",
    "\n",
    "def train_val_loss_plot(axes, metrics, labels, meta, fontsize=8, xlab='', ylab='Loss'):\n",
    "    nrows = axes.shape[0] if hasattr(axes, 'shape') else 1\n",
    "    flat_axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    for i, ax in enumerate(flat_axes[:len(metrics)]):\n",
    "        ax.plot(metrics[i]['train'], label='Train')\n",
    "        ax.plot(metrics[i]['test'], label='Validation')\n",
    "        ax.set_title(f\"{labels[i]}\\n{meta[i]}\", fontsize=fontsize)\n",
    "        ax.set_xlabel(xlab)\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.legend()\n",
    "\n",
    "def train_val_f1_plot(axes, metrics, labels, meta, target_labs, fontsize=8, xlab='', ylab='F1'):\n",
    "    nrows = axes.shape[0] if hasattr(axes, 'shape') else 1\n",
    "    flat_axes = axes.flatten() if hasattr(axes, 'flatten') else axes\n",
    "    for i, ax in enumerate(flat_axes[:len(metrics)]):\n",
    "        ax.plot(metrics[i]['f1'], label=target_labs)\n",
    "        ax.set_title(f\"{labels[i]}\\n{meta[i]}\", fontsize=fontsize)\n",
    "        ax.set_xlabel(xlab)\n",
    "        ax.set_ylabel(ylab)\n",
    "        ax.legend()\n",
    "\n",
    "# Plot loss\n",
    "fig, axes = make_canvas(len(histories))\n",
    "train_val_loss_plot(axes, histories, storm_labels, metadatas)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 scores\n",
    "fig, axes = make_canvas(len(histories))\n",
    "train_val_f1_plot(axes, histories, storm_labels, metadatas, target_labs='F1')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
