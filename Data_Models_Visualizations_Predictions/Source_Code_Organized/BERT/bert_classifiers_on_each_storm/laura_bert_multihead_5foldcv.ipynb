{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)\n",
    "\n",
    "# Load tokenizer and base BERT model\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# ------------------ UTILITY FUNCTIONS ------------------\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n",
    "\n",
    "# ------------------ DATASET CLASS ------------------\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# ------------------ MODEL ------------------\n",
    "\n",
    "class BERTDeepMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, num_targets=11, hidden_dim=256):\n",
    "        super(BERTDeepMultiHeadClassifier, self).__init__()\n",
    "        self.bert = model_bert\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.drop(pooled_output)\n",
    "        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n",
    "\n",
    "# ------------------ TRAINING LOOP ------------------\n",
    "\n",
    "def train_negative_only_model_kfold(grouped_tweets, target_list=[], max_len=512, batch_size=24, num_epochs=10, num_folds=5):\n",
    "    target_list = get_target_list(target_list)\n",
    "    tweet_dict = {int(name): group['text'] for name, group in grouped_tweets}\n",
    "\n",
    "    texts = ['\\n'.join(group.to_list()) for group in tweet_dict.values()]\n",
    "    \n",
    "    # üîß All-zero labels for negative-only setup\n",
    "    all_zero_label = np.zeros(len(target_list), dtype=np.float32)\n",
    "    labels = [all_zero_label.copy() for _ in texts]\n",
    "\n",
    "    save_path_laura = 'Training_results/Laura_NegativeOnly_KFold/'\n",
    "    os.makedirs(save_path_laura, exist_ok=True)\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    best_model_paths = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(texts)):\n",
    "        print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        X_train = [texts[i] for i in train_idx]\n",
    "        y_train = [labels[i] for i in train_idx]\n",
    "        X_val = [texts[i] for i in val_idx]\n",
    "        y_val = [labels[i] for i in val_idx]  # Explicitly reassign validation labels as all-zero\n",
    "\n",
    "        train_dataset = CustomDataset(X_train, y_train, tokenizer, max_len)\n",
    "        val_dataset = CustomDataset(X_val, y_val, tokenizer, max_len)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "        criterion = [nn.BCELoss() for _ in range(len(target_list))]\n",
    "\n",
    "        best_f1 = 0.0\n",
    "        best_model_path = os.path.join(save_path_laura, f\"bert_model_laura_neg_fold{fold + 1}.pth\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} Training\"):\n",
    "                optimizer.zero_grad()\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = sum(c(outputs[:, i], labels[:, i]) for i, c in enumerate(criterion))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=f\"Fold {fold+1} Epoch {epoch+1} Validation\"):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['labels'].to(device)\n",
    "\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    loss = sum(c(outputs[:, i], labels[:, i]) for i, c in enumerate(criterion))\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    all_preds.append(outputs.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            all_preds = np.vstack(all_preds)\n",
    "            all_labels = np.vstack(all_labels)\n",
    "            f1 = [f1_score(all_labels[:, i], all_preds[:, i] > 0.5, zero_division=0) for i in range(len(target_list))]\n",
    "            mean_f1 = np.mean(f1)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss={train_loss/len(train_loader):.4f}, Val Loss={val_loss/len(val_loader):.4f}, Mean F1={mean_f1:.4f}\")\n",
    "\n",
    "            if mean_f1 > best_f1:\n",
    "                best_f1 = mean_f1\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"Saved best model for Fold {fold+1} with F1={best_f1:.4f}\")\n",
    "\n",
    "        #Save final model if F1 never improved\n",
    "        if best_f1 == 0.0:\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"‚ö†Ô∏è F1 never improved for Fold {fold+1}. Final model saved anyway.\")\n",
    "\n",
    "        best_model_paths.append(best_model_path)\n",
    "\n",
    "    return best_model_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ EXECUTION ------------------\n",
    "\n",
    "size_threshold = 80\n",
    "tweets2 = pd.read_csv('organized_with_zipcode.csv')\n",
    "tweets_laura = tweets2[tweets2.storm_name == 'laura']\n",
    "bboxes_useful = tweets_laura.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_laura = tweets_laura.loc[((tweets_laura.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "tweet_grouped_laura = tweets_laura.groupby('zip_code')\n",
    "\n",
    "# Train model on negative-only data\n",
    "best_model_path = train_negative_only_model_kfold(tweet_grouped_laura, max_len=512, batch_size=24, num_epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
