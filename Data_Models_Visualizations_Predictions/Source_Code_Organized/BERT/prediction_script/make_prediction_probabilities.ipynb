{"cells":[{"cell_type":"markdown","id":"32aa167f-ae08-45f6-9acb-7a55703f0c9d","metadata":{"id":"32aa167f-ae08-45f6-9acb-7a55703f0c9d"},"source":["## Predict on Harvey"]},{"cell_type":"code","execution_count":null,"id":"ccac81ba-8832-49fb-9d51-4cbc1f3bb3a0","metadata":{"id":"ccac81ba-8832-49fb-9d51-4cbc1f3bb3a0","outputId":"f7eb903b-60f9-40b5-8b4a-5ec06288597a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_18096\\1686881798.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(path))\n","Making predictions: 100%|███████████████████████████████████████████████████████████| 20/20 [00:48<00:00,  2.43s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Mean predictions saved to Training_results/Harvey_5foldcv_BERT_Multihead/all_zipcodes_with_tweets_bert_mean_predictions.xlsx\n","Binary predictions saved to Training_results/Harvey_5foldcv_BERT_Multihead/all_zipcodes_with_tweets_bert_binary_predictions.xlsx\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import copy\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(64)\n","\n","# Load the saved models\n","save_path = 'Training_results/Harvey_5foldcv_BERT_Multihead/'\n","max_length = 256\n","batchsize = 48\n","saved_model_paths = [os.path.join(save_path, f\"best_bert_model_fold_{i+1}.pth\") for i in range(5)]\n","models = []\n","\n","# Load BERT tokenizer and model\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n","\n","# Get the target list\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 3959\n","    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n","    dlat, dlon = lat2 - lat1, lon2 - lon1\n","    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n","    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n","    return R * c\n","\n","def get_box_area(lat1, lon1, lat2, lon2):\n","    side1 = haversine(lat1, lon1, lat1, lon2)\n","    side2 = haversine(lat1, lon1, lat2, lon1)\n","    return side1 * side2\n","\n","# Get the target list if not provided\n","def get_target_list(target_list=[]):\n","    if not target_list:\n","        target_list = [\n","            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","            'replacementAssistanceEligible', 'personalPropertyEligible'\n","        ]\n","    return target_list\n","\n","def grouped_tweets_to_dict(tweet_grouped):\n","    return {int(name): group['text'] for name, group in tweet_grouped}\n","\n","# CustomDataset without labels (for inference only)\n","class InferenceDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(0),\n","            'attention_mask': encoding['attention_mask'].squeeze(0)\n","        }\n","\n","class BERTDeepMultiHeadClassifier(nn.Module):\n","    def __init__(self, num_targets=11, hidden_dim=256):\n","        super(BERTDeepMultiHeadClassifier, self).__init__()\n","        self.bert = model_bert\n","        self.drop = nn.Dropout(0.3)\n","\n","        self.heads = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, 1)\n","            ) for _ in range(num_targets)\n","        ])\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = self.drop(pooled_output)\n","        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n","\n","# Load all trained models\n","for path in saved_model_paths:\n","    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    models.append(model)\n","\n","size_threshold = 80\n","\n","tweets_harvey2 = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/tweets/harvey_corrected.csv')\n","tweets_harvey2.rename(columns={'zipcode': 'zip_code'}, inplace=True)\n","\n","bboxes_useful = tweets_harvey2.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n","bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n","tweets_harvey = tweets_harvey2.loc[((tweets_harvey2.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n","tweets_harvey.loc[:, 'zip_code'] = tweets_harvey['zip_code'].apply(int)\n","tweet_grouped_harvey = tweets_harvey.groupby('zip_code')\n","\n","tweet_dict = {int(name): '\\n'.join(group['text'].to_list()) for name, group in tweet_grouped_harvey}\n","\n","# Prepare data\n","zip_codes = list(tweet_dict.keys())\n","texts = [tweet_dict[zc] for zc in zip_codes]\n","\n","# Create inference dataset and loader\n","inference_dataset = InferenceDataset(texts=texts, tokenizer=tokenizer, max_len=max_length)\n","inference_loader = DataLoader(inference_dataset, batch_size=batchsize, shuffle=False)\n","\n","# Store predictions\n","all_preds = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(inference_loader, desc=\"Making predictions\", unit=\"batch\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        fold_preds = []\n","        for model in models:\n","            outputs = model(input_ids, attention_mask)\n","            fold_preds.append(outputs.cpu().numpy())\n","\n","        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n","        all_preds.append(fold_preds)\n","\n","# Combine predictions\n","all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n","mean_preds = all_preds.mean(axis=0)            # Shape: (total_samples, num_targets)\n","binary_preds = (mean_preds > 0.5).astype(int)\n","\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","# Save to files\n","mean_preds_df = pd.DataFrame(mean_preds, columns=target_list)\n","mean_preds_df.insert(0, \"zip_code\", zip_codes)\n","mean_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_mean_predictions.xlsx\")\n","mean_preds_df.to_excel(mean_preds_path, index=False)\n","\n","binary_preds_df = pd.DataFrame(binary_preds, columns=target_list)\n","binary_preds_df.insert(0, \"zip_code\", zip_codes)\n","binary_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_binary_predictions.xlsx\")\n","binary_preds_df.to_excel(binary_preds_path, index=False)\n","\n","print(f\"Mean predictions saved to {mean_preds_path}\")\n","print(f\"Binary predictions saved to {binary_preds_path}\")"]},{"cell_type":"code","execution_count":null,"id":"1e402b7f-75cf-4dae-8b26-8fc541b196a9","metadata":{"id":"1e402b7f-75cf-4dae-8b26-8fc541b196a9"},"outputs":[],"source":["for model in models:\n","    del model"]},{"cell_type":"markdown","id":"363c5962-0155-42b7-a7ea-b8feaa404041","metadata":{"id":"363c5962-0155-42b7-a7ea-b8feaa404041"},"source":["## Predict on Beryl"]},{"cell_type":"code","execution_count":null,"id":"5559523f-700b-4a81-b075-de70a4e3f530","metadata":{"id":"5559523f-700b-4a81-b075-de70a4e3f530","outputId":"2f35a42a-a9cb-48a2-df56-3a22dce5d494"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_18096\\2878012014.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(path))\n","Making predictions: 100%|█████████████████████████████████████████████████████████████| 6/6 [00:12<00:00,  2.15s/batch]"]},{"name":"stdout","output_type":"stream","text":["Mean predictions saved to Training_results/Beryl_5foldcv_BERT_Multihead_256/all_zipcodes_with_tweets_bert_mean_predictions.xlsx\n","Binary predictions saved to Training_results/Beryl_5foldcv_BERT_Multihead_256/all_zipcodes_with_tweets_bert_binary_predictions.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import copy\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(64)\n","\n","# Load the saved models\n","save_path = 'Training_results/Beryl_5foldcv_BERT_Multihead_256/'\n","max_length = 256\n","batchsize = 48\n","saved_model_paths = [os.path.join(save_path, f\"best_bert_model_fold_{i+1}.pth\") for i in range(5)]\n","models = []\n","\n","# Load BERT tokenizer and model\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n","\n","# Get the target list\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 3959\n","    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n","    dlat, dlon = lat2 - lat1, lon2 - lon1\n","    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n","    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n","    return R * c\n","\n","def get_box_area(lat1, lon1, lat2, lon2):\n","    side1 = haversine(lat1, lon1, lat1, lon2)\n","    side2 = haversine(lat1, lon1, lat2, lon1)\n","    return side1 * side2\n","\n","# Get the target list if not provided\n","def get_target_list(target_list=[]):\n","    if not target_list:\n","        target_list = [\n","            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","            'replacementAssistanceEligible', 'personalPropertyEligible'\n","        ]\n","    return target_list\n","\n","def grouped_tweets_to_dict(tweet_grouped):\n","    return {int(name): group['text'] for name, group in tweet_grouped}\n","\n","# CustomDataset without labels (for inference only)\n","class InferenceDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(0),\n","            'attention_mask': encoding['attention_mask'].squeeze(0)\n","        }\n","\n","class BERTDeepMultiHeadClassifier(nn.Module):\n","    def __init__(self, num_targets=11, hidden_dim=256):\n","        super(BERTDeepMultiHeadClassifier, self).__init__()\n","        self.bert = model_bert\n","        self.drop = nn.Dropout(0.3)\n","\n","        self.heads = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, 1)\n","            ) for _ in range(num_targets)\n","        ])\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = self.drop(pooled_output)\n","        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n","\n","# Load all trained models\n","for path in saved_model_paths:\n","    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    models.append(model)\n","\n","size_threshold = 80\n","\n","tweets2 = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/tweets/organized_with_zipcode.csv')  # read in massive tweets dataset\n","tweets_beryl = tweets2[(tweets2.storm_name == 'beryl')]\n","bboxes_useful = tweets_beryl.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n","bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n","tweets_beryl = tweets_beryl.loc[((tweets_beryl.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)),:]  # since i'm using iloc i think the indices will match\n","tweet_grouped_beryl = tweets_beryl.groupby('zip_code')\n","\n","tweet_dict = {int(name): '\\n'.join(group['text'].to_list()) for name, group in tweet_grouped_beryl}\n","\n","# Prepare data\n","zip_codes = list(tweet_dict.keys())\n","texts = [tweet_dict[zc] for zc in zip_codes]\n","\n","# Create inference dataset and loader\n","inference_dataset = InferenceDataset(texts=texts, tokenizer=tokenizer, max_len=max_length)\n","inference_loader = DataLoader(inference_dataset, batch_size=batchsize, shuffle=False)\n","\n","# Store predictions\n","all_preds = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(inference_loader, desc=\"Making predictions\", unit=\"batch\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        fold_preds = []\n","        for model in models:\n","            outputs = model(input_ids, attention_mask)\n","            fold_preds.append(outputs.cpu().numpy())\n","\n","        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n","        all_preds.append(fold_preds)\n","\n","# Combine predictions\n","all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n","mean_preds = all_preds.mean(axis=0)            # Shape: (total_samples, num_targets)\n","binary_preds = (mean_preds > 0.5).astype(int)\n","\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","# Save to files\n","mean_preds_df = pd.DataFrame(mean_preds, columns=target_list)\n","mean_preds_df.insert(0, \"zip_code\", zip_codes)\n","mean_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_mean_predictions.xlsx\")\n","mean_preds_df.to_excel(mean_preds_path, index=False)\n","\n","binary_preds_df = pd.DataFrame(binary_preds, columns=target_list)\n","binary_preds_df.insert(0, \"zip_code\", zip_codes)\n","binary_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_binary_predictions.xlsx\")\n","binary_preds_df.to_excel(binary_preds_path, index=False)\n","\n","print(f\"Mean predictions saved to {mean_preds_path}\")\n","print(f\"Binary predictions saved to {binary_preds_path}\")"]},{"cell_type":"code","execution_count":null,"id":"1b7b9b28-b38a-420e-8b1f-202d417177b3","metadata":{"id":"1b7b9b28-b38a-420e-8b1f-202d417177b3"},"outputs":[],"source":["for model in models:\n","    del model"]},{"cell_type":"markdown","id":"661cff23-1b5e-438b-ab53-c8fda2971f15","metadata":{"id":"661cff23-1b5e-438b-ab53-c8fda2971f15"},"source":["## Predict on Imelda"]},{"cell_type":"code","execution_count":null,"id":"0edd1455-9357-4b19-9c8d-2450b2f4a564","metadata":{"id":"0edd1455-9357-4b19-9c8d-2450b2f4a564","outputId":"57f743e6-335a-4aaa-9e38-a3a5c8276700"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\udays\\AppData\\Local\\Temp\\ipykernel_18096\\3976675152.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(path))\n","Making predictions: 100%|███████████████████████████████████████████████████████████| 19/19 [00:55<00:00,  2.90s/batch]\n"]},{"name":"stdout","output_type":"stream","text":["Mean predictions saved to Training_results/Imelda_5foldcv_BERT_Multihead_256/all_zipcodes_with_tweets_bert_mean_predictions.xlsx\n","Binary predictions saved to Training_results/Imelda_5foldcv_BERT_Multihead_256/all_zipcodes_with_tweets_bert_binary_predictions.xlsx\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import transformers\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import copy\n","import math\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(64)\n","\n","# Load the saved models\n","save_path = 'Training_results/Imelda_5foldcv_BERT_Multihead_256/'\n","max_length = 256\n","batchsize = 48\n","saved_model_paths = [os.path.join(save_path, f\"best_bert_model_fold_{i+1}.pth\") for i in range(5)]\n","models = []\n","\n","# Load BERT tokenizer and model\n","tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n","model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n","\n","# Get the target list\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 3959\n","    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n","    dlat, dlon = lat2 - lat1, lon2 - lon1\n","    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n","    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n","    return R * c\n","\n","def get_box_area(lat1, lon1, lat2, lon2):\n","    side1 = haversine(lat1, lon1, lat1, lon2)\n","    side2 = haversine(lat1, lon1, lat2, lon1)\n","    return side1 * side2\n","\n","# Get the target list if not provided\n","def get_target_list(target_list=[]):\n","    if not target_list:\n","        target_list = [\n","            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","            'replacementAssistanceEligible', 'personalPropertyEligible'\n","        ]\n","    return target_list\n","\n","def grouped_tweets_to_dict(tweet_grouped):\n","    return {int(name): group['text'] for name, group in tweet_grouped}\n","\n","# CustomDataset without labels (for inference only)\n","class InferenceDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_len=512):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].squeeze(0),\n","            'attention_mask': encoding['attention_mask'].squeeze(0)\n","        }\n","\n","class BERTDeepMultiHeadClassifier(nn.Module):\n","    def __init__(self, num_targets=11, hidden_dim=256):\n","        super(BERTDeepMultiHeadClassifier, self).__init__()\n","        self.bert = model_bert\n","        self.drop = nn.Dropout(0.3)\n","\n","        self.heads = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, 1)\n","            ) for _ in range(num_targets)\n","        ])\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        x = self.drop(pooled_output)\n","        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n","\n","# Load all trained models\n","for path in saved_model_paths:\n","    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n","    model.load_state_dict(torch.load(path))\n","    model.eval()\n","    models.append(model)\n","\n","size_threshold = 80\n","\n","tweets2 = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/tweets/organized_with_zipcode.csv')  # read in massive tweets dataset\n","tweets_imelda = tweets2[(tweets2.storm_name == 'imelda')]\n","bboxes_useful = tweets_imelda.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n","bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n","tweets_imelda = tweets_imelda.loc[((tweets_imelda.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)),:]  # since i'm using iloc i think the indices will match\n","tweet_grouped_imelda = tweets_imelda.groupby('zip_code')\n","\n","tweet_dict = {int(name): '\\n'.join(group['text'].to_list()) for name, group in tweet_grouped_imelda}\n","\n","# Prepare data\n","zip_codes = list(tweet_dict.keys())\n","texts = [tweet_dict[zc] for zc in zip_codes]\n","\n","# Create inference dataset and loader\n","inference_dataset = InferenceDataset(texts=texts, tokenizer=tokenizer, max_len=max_length)\n","inference_loader = DataLoader(inference_dataset, batch_size=batchsize, shuffle=False)\n","\n","# Store predictions\n","all_preds = []\n","\n","with torch.no_grad():\n","    for batch in tqdm(inference_loader, desc=\"Making predictions\", unit=\"batch\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","\n","        fold_preds = []\n","        for model in models:\n","            outputs = model(input_ids, attention_mask)\n","            fold_preds.append(outputs.cpu().numpy())\n","\n","        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n","        all_preds.append(fold_preds)\n","\n","# Combine predictions\n","all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n","mean_preds = all_preds.mean(axis=0)            # Shape: (total_samples, num_targets)\n","binary_preds = (mean_preds > 0.5).astype(int)\n","\n","target_list = [\n","    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n","    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n","    'replacementAssistanceEligible', 'personalPropertyEligible'\n","]\n","\n","# Save to files\n","mean_preds_df = pd.DataFrame(mean_preds, columns=target_list)\n","mean_preds_df.insert(0, \"zip_code\", zip_codes)\n","mean_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_mean_predictions.xlsx\")\n","mean_preds_df.to_excel(mean_preds_path, index=False)\n","\n","binary_preds_df = pd.DataFrame(binary_preds, columns=target_list)\n","binary_preds_df.insert(0, \"zip_code\", zip_codes)\n","binary_preds_path = os.path.join(save_path, \"all_zipcodes_with_tweets_bert_binary_predictions.xlsx\")\n","binary_preds_df.to_excel(binary_preds_path, index=False)\n","\n","print(f\"Mean predictions saved to {mean_preds_path}\")\n","print(f\"Binary predictions saved to {binary_preds_path}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}