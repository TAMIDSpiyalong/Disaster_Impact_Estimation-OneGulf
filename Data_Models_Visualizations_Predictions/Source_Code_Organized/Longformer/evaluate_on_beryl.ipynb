{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d372142-7c8e-468a-87e9-85e08613c4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making predictions: 100%|██████████| 6/6 [01:16<00:00, 12.75s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean predictions saved to Training_results/Laura_NegativeOnly/laura_bert_on_beryl_mean_predictions.xlsx\n",
      "Binary predictions saved to Training_results/Laura_NegativeOnly/laura_bert_on_beryl_binary_predictions.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)\n",
    "\n",
    "# Load the saved models\n",
    "save_path = 'Training_results/Laura_NegativeOnly/'\n",
    "max_length = 256\n",
    "batchsize = 48\n",
    "saved_model_paths = [os.path.join(save_path, f\"final_bert_model_laura_neg.pth\") for i in range(5)]\n",
    "models = []\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the target list\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "# Get the target list if not provided\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n",
    "\n",
    "def grouped_tweets_to_dict(tweet_grouped):\n",
    "    return {int(name): group['text'] for name, group in tweet_grouped}\n",
    "\n",
    "# CustomDataset without labels (for inference only)\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0)\n",
    "        }\n",
    "\n",
    "class BERTDeepMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, num_targets=11, hidden_dim=256):\n",
    "        super(BERTDeepMultiHeadClassifier, self).__init__()\n",
    "        self.bert = model_bert\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.drop(pooled_output)\n",
    "        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n",
    "\n",
    "map_location = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for path in saved_model_paths:\n",
    "    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(map_location)\n",
    "    model.load_state_dict(torch.load(path, map_location=map_location))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "size_threshold = 80\n",
    "\n",
    "tweets2 = pd.read_csv('organized_with_zipcode.csv')  # read in massive tweets dataset\n",
    "tweets_beryl = tweets2[(tweets2.storm_name == 'beryl')]\n",
    "bboxes_useful = tweets_beryl.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_beryl = tweets_beryl.loc[((tweets_beryl.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)),:]  # since i'm using iloc i think the indices will match\n",
    "tweet_grouped_beryl = tweets_beryl.groupby('zip_code')\n",
    "\n",
    "tweet_dict = {int(name): '\\n'.join(group['text'].to_list()) for name, group in tweet_grouped_beryl}\n",
    "\n",
    "# Prepare data\n",
    "zip_codes = list(tweet_dict.keys())\n",
    "texts = [tweet_dict[zc] for zc in zip_codes]\n",
    "\n",
    "# Create inference dataset and loader\n",
    "inference_dataset = InferenceDataset(texts=texts, tokenizer=tokenizer, max_len=max_length)\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "# Store predictions\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(inference_loader, desc=\"Making predictions\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        fold_preds = []\n",
    "        for model in models:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            fold_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "# Combine predictions\n",
    "all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n",
    "mean_preds = all_preds.mean(axis=0)            # Shape: (total_samples, num_targets)\n",
    "binary_preds = (mean_preds > 0.5).astype(int)\n",
    "\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage',\n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible',\n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "# Save to files\n",
    "mean_preds_df = pd.DataFrame(mean_preds, columns=target_list)\n",
    "mean_preds_df.insert(0, \"zip_code\", zip_codes)\n",
    "mean_preds_path = os.path.join(save_path, \"laura_bert_on_beryl_mean_predictions.xlsx\")\n",
    "mean_preds_df.to_excel(mean_preds_path, index=False)\n",
    "\n",
    "binary_preds_df = pd.DataFrame(binary_preds, columns=target_list)\n",
    "binary_preds_df.insert(0, \"zip_code\", zip_codes)\n",
    "binary_preds_path = os.path.join(save_path, \"laura_bert_on_beryl_binary_predictions.xlsx\")\n",
    "binary_preds_df.to_excel(binary_preds_path, index=False)\n",
    "\n",
    "print(f\"Mean predictions saved to {mean_preds_path}\")\n",
    "print(f\"Binary predictions saved to {binary_preds_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f486177-cd40-4b07-a912-f3a138c1d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/89/9glrgdc55d79wbzwgyfj0w7r0000gn/T/ipykernel_84212/811228265.py:126: DtypeWarning: Columns (9,13,28,46,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  targets_beryl = pd.read_csv('disaster_4798.csv')\n",
      "Making predictions:   0%|          | 0/4 [00:00<?, ?batch/s]/var/folders/89/9glrgdc55d79wbzwgyfj0w7r0000gn/T/ipykernel_84212/811228265.py:85: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'labels': torch.tensor(label, dtype=torch.float32)\n",
      "Making predictions: 100%|██████████| 4/4 [00:50<00:00, 12.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to Training_results/Laura_NegativeOnly/bert_metrics_on_beryl.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)\n",
    "\n",
    "# Load the saved models\n",
    "save_path = 'Training_results/Laura_NegativeOnly/'\n",
    "saved_model_paths = [os.path.join(save_path, f\"final_bert_model_laura_neg.pth\") for i in range(5)]\n",
    "models = []\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Get the target list\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959  \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "# Get the target list if not provided\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n",
    "\n",
    "# Define dataset class to handle tokenization and data loading\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class BERTDeepMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, num_targets=11, hidden_dim=256):\n",
    "        super(BERTDeepMultiHeadClassifier, self).__init__()\n",
    "        self.bert = model_bert\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.drop(pooled_output)\n",
    "        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n",
    "\n",
    "# Load all trained models\n",
    "map_location = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for path in saved_model_paths:\n",
    "    model = BERTDeepMultiHeadClassifier(num_targets=len(target_list)).to(map_location)\n",
    "    model.load_state_dict(torch.load(path, map_location=map_location))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "size_threshold = 80\n",
    "\n",
    "tweets2 = pd.read_csv('organized_with_zipcode.csv')  # read in massive tweets dataset\n",
    "tweets_beryl = tweets2[(tweets2.storm_name == 'beryl')]\n",
    "bboxes_useful = tweets_beryl.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_beryl = tweets_beryl.loc[((tweets_beryl.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)),:]  # since i'm using iloc i think the indices will match\n",
    "tweet_grouped_beryl = tweets_beryl.groupby('zip_code')\n",
    "\n",
    "targets_beryl = pd.read_csv('disaster_4798.csv')\n",
    "target_grouped_beryl = targets_beryl.groupby('damagedZipCode')\n",
    "    \n",
    "tweet_dict = {int(name): group['text'] for name, group in tweet_grouped_beryl}\n",
    "target_dict = {int(name): group[target_list] for name, group in target_grouped_beryl}\n",
    "\n",
    "intersecting_zips = list(set(target_dict.keys()) & set(tweet_dict.keys()))\n",
    "paired_data = {\n",
    "    name: [target_dict[name].sum().apply(lambda x: 1 if x > 0 else 0), tweet_dict[name]] for name in intersecting_zips\n",
    "}\n",
    "\n",
    "# Prepare dataset for inference\n",
    "texts = ['\\n'.join(v[1].to_list()) for v in paired_data.values()]\n",
    "labels_ = [v[0] for v in paired_data.values()]\n",
    "zip_codes = list(paired_data.keys())\n",
    "\n",
    "test_dataset = CustomDataset(texts, labels_, tokenizer, max_len=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "# Store predictions\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Making predictions\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        fold_preds = []\n",
    "        for model in models:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            fold_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "# Convert list to numpy array\n",
    "all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n",
    "mean_preds = all_preds.mean(axis=0)  # Mean prediction probabilities (Shape: total_samples, num_targets)\n",
    "binary_preds = (mean_preds > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Convert labels to numpy array\n",
    "true_labels = np.vstack(labels_)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "# metrics_dict = {'Target': target_list, 'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': []}\n",
    "metrics_dict = {'Target': target_list, 'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': [], 'Support': []}\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    metrics_dict['Accuracy'].append(accuracy_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['F1_Score'].append(f1_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Precision'].append(precision_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Recall'].append(recall_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Support'].append(int(true_labels[:, i].sum()))\n",
    "\n",
    "# Compute and append average metrics\n",
    "metrics_dict['Target'].append('Average')\n",
    "metrics_dict['Accuracy'].append(np.mean(metrics_dict['Accuracy']))\n",
    "metrics_dict['F1_Score'].append(np.mean(metrics_dict['F1_Score']))\n",
    "metrics_dict['Precision'].append(np.mean(metrics_dict['Precision']))\n",
    "metrics_dict['Recall'].append(np.mean(metrics_dict['Recall']))\n",
    "metrics_dict['Support'].append('–')\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "metrics_df_path = os.path.join(save_path, \"bert_metrics_on_beryl.xlsx\")\n",
    "metrics_df.to_excel(metrics_df_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to {metrics_df_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
