{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important imports and deviee setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Saved Models - Choose one of the three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load Imelda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'Training_results/Imelda_5foldcv_Longformer_Multihead/'\n",
    "saved_model_paths = [os.path.join(save_path, f\"best_longformer_model_fold_{i+1}.pth\") for i in range(5)]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load Beryl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'Training_results/Beryl_5foldcv_Longformer_Multihead/'\n",
    "saved_model_paths = [os.path.join(save_path, f\"best_longformer_model_fold_{i+1}.pth\") for i in range(5)]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to load Harvey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'Training_results/Harvey_5foldcv_Longformer_Multihead/'\n",
    "saved_model_paths = [os.path.join(save_path, f\"best_longformer_model_fold_{i+1}.pth\") for i in range(5)]\n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare Tokenizer and other important functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "model_longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Get the target list\n",
    "target_list = [\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "]\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959  \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "# Get the target list if not provided\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dataset class to handle tokenization and data loading\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenizing the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "class LongformerDeepMultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, num_targets=11, hidden_dim=256):\n",
    "        super(LongformerDeepMultiHeadClassifier, self).__init__()\n",
    "        self.longformer = model_longformer\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.longformer.config.hidden_size, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, 1)\n",
    "            ) for _ in range(num_targets)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Longformer doesn't use pooler_output, so use mean pooling over the last hidden state\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = sequence_output.mean(dim=1)\n",
    "        x = self.drop(pooled_output)\n",
    "        return torch.cat([torch.sigmoid(head(x)) for head in self.heads], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Trained Models and perform Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all trained models\n",
    "for path in saved_model_paths:\n",
    "    model = LongformerDeepMultiHeadClassifier(num_targets=len(target_list)).to(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "size_threshold = 80\n",
    "\n",
    "tweets2 = pd.read_csv('organized_with_zipcode.csv')  # read in massive tweets dataset\n",
    "tweets_imelda = tweets2[(tweets2.storm_name == 'imelda')]\n",
    "bboxes_useful = tweets_imelda.place_bbox.apply(lambda x: [[float(i.strip('(').strip(')')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_imelda = tweets_imelda.loc[((tweets_imelda.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)),:]  # since i'm using iloc i think the indices will match\n",
    "tweet_grouped_imelda = tweets_imelda.groupby('zip_code')\n",
    "\n",
    "targets_imelda = pd.read_csv('disaster_4466.csv')\n",
    "target_grouped_imelda = targets_imelda.groupby('damagedZipCode')\n",
    "    \n",
    "tweet_dict = {int(name): group['text'] for name, group in tweet_grouped_imelda}\n",
    "target_dict = {int(name): group[target_list] for name, group in target_grouped_imelda}\n",
    "\n",
    "intersecting_zips = list(set(target_dict.keys()) & set(tweet_dict.keys()))\n",
    "paired_data = {\n",
    "    name: [target_dict[name].sum().apply(lambda x: 1 if x > 0 else 0), tweet_dict[name]] for name in intersecting_zips\n",
    "}\n",
    "\n",
    "# Prepare dataset for inference\n",
    "texts = ['\\n'.join(v[1].to_list()) for v in paired_data.values()]\n",
    "labels_ = [v[0] for v in paired_data.values()]\n",
    "zip_codes = list(paired_data.keys())\n",
    "\n",
    "test_dataset = CustomDataset(texts, labels_, tokenizer, max_len=512)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
    "\n",
    "# Store predictions\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Making predictions\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        fold_preds = []\n",
    "        for model in models:\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            fold_preds.append(outputs.cpu().numpy())\n",
    "\n",
    "        fold_preds = np.stack(fold_preds, axis=0)  # Shape: (num_models, batch_size, num_targets)\n",
    "        all_preds.append(fold_preds)\n",
    "\n",
    "# Convert list to numpy array\n",
    "all_preds = np.concatenate(all_preds, axis=1)  # Shape: (num_models, total_samples, num_targets)\n",
    "mean_preds = all_preds.mean(axis=0)  # Mean prediction probabilities (Shape: total_samples, num_targets)\n",
    "binary_preds = (mean_preds > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Convert labels to numpy array\n",
    "true_labels = np.vstack(labels_)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "# metrics_dict = {'Target': target_list, 'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': []}\n",
    "metrics_dict = {'Target': target_list, 'Accuracy': [], 'F1_Score': [], 'Precision': [], 'Recall': [], 'Support': []}\n",
    "\n",
    "for i in range(len(target_list)):\n",
    "    metrics_dict['Accuracy'].append(accuracy_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['F1_Score'].append(f1_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Precision'].append(precision_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Recall'].append(recall_score(true_labels[:, i], binary_preds[:, i]))\n",
    "    metrics_dict['Support'].append(int(true_labels[:, i].sum()))\n",
    "\n",
    "# Compute and append average metrics\n",
    "metrics_dict['Target'].append('Average')\n",
    "metrics_dict['Accuracy'].append(np.mean(metrics_dict['Accuracy']))\n",
    "metrics_dict['F1_Score'].append(np.mean(metrics_dict['F1_Score']))\n",
    "metrics_dict['Precision'].append(np.mean(metrics_dict['Precision']))\n",
    "metrics_dict['Recall'].append(np.mean(metrics_dict['Recall']))\n",
    "metrics_dict['Support'].append('â€“')\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "metrics_df = pd.DataFrame(metrics_dict)\n",
    "metrics_df_path = os.path.join(save_path, \"longformer_metrics_on_imelda.xlsx\")\n",
    "metrics_df.to_excel(metrics_df_path, index=False)\n",
    "\n",
    "print(f\"Metrics saved to {metrics_df_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
