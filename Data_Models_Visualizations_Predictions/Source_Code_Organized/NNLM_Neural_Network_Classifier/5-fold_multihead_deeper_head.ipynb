{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7820b7b-2115-4fa3-9d3c-c976502b09a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(64)\n",
    "\n",
    "save_path = 'Training_results/5foldcv_BCE_Multihead_Deep_Sigmoid/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 3959  \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def get_box_area(lat1, lon1, lat2, lon2):\n",
    "    side1 = haversine(lat1, lon1, lat1, lon2)\n",
    "    side2 = haversine(lat1, lon1, lat2, lon1)\n",
    "    return side1 * side2\n",
    "\n",
    "hub_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/nnlm-en-dim128/2\", \n",
    "    input_shape=[], dtype=tf.string, trainable=True\n",
    ")\n",
    "\n",
    "def get_target_list(target_list=[]):\n",
    "    if not target_list:\n",
    "        target_list = [\n",
    "            'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "            'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "            'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "        ]\n",
    "    return target_list\n",
    "\n",
    "\n",
    "def grouped_tweets_to_dict(tweet_grouped):\n",
    "    return {int(name): group['text'] for name, group in tweet_grouped}\n",
    "\n",
    "\n",
    "def grouped_targets_to_dict(target_grouped, target_list):\n",
    "    return {int(name): group[target_list] for name, group in target_grouped}\n",
    "\n",
    "\n",
    "def get_intersection_keys(tweet_dict, target_dict):\n",
    "    return list(set(target_dict.keys()) & set(tweet_dict.keys()))\n",
    "\n",
    "\n",
    "def pair_data_together(intersection, tweet_dict, target_dict):\n",
    "    return {name: [target_dict[name].sum().apply(lambda x: 1 if x > 0 else 0), tweet_dict[name]] for name in intersection}\n",
    "\n",
    "\n",
    "def pair_data_together_target_non_binary(intersection, tweet_dict, target_dict):\n",
    "    return {name: [target_dict[name].sum(), tweet_dict[name]] for name in intersection}\n",
    "\n",
    "\n",
    "def get_class_weights(paired_dict):\n",
    "    pos_counts = np.zeros(len(paired_dict[list(paired_dict.keys())[0]][0]))\n",
    "    for i in paired_dict.keys():\n",
    "        pos_counts += paired_dict[i][0]\n",
    "    neg_counts = len(paired_dict.keys()) - pos_counts\n",
    "    return neg_counts / pos_counts\n",
    "\n",
    "\n",
    "def load_embedding_model(emb_model_str):\n",
    "    model_path = hub.resolve(emb_model_str)\n",
    "    return hub.KerasLayer(model_path, input_shape=[], dtype=tf.string, trainable=True)\n",
    "\n",
    "\n",
    "def paired_data_to_Xy(paired_data, hub_layer):\n",
    "    X, y = [], []\n",
    "    for k, v in paired_data.items():\n",
    "        y.append(np.array(v[0]))\n",
    "        text = '\\n'.join(v[1].to_list())\n",
    "        vector = hub_layer([text]).numpy()\n",
    "        X.append(vector)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def tweet_data_to_X_dict(tweet_data, hub_layer):\n",
    "\n",
    "    X_dict = {}\n",
    "    for k, v in tweet_data.items():\n",
    "        \n",
    "        X = []\n",
    "        text = '\\n'.join(v.to_list())\n",
    "        vector = hub_layer([text]).numpy()\n",
    "        X.append(vector)\n",
    "        X_dict[k] = np.array(X)\n",
    "        \n",
    "    return X_dict\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.features[idx][0], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "class MultiHeadSimplerNet(nn.Module):\n",
    "    def __init__(self, in_out_degrees, num_heads):\n",
    "        super(MultiHeadSimplerNet, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            *[nn.Linear(in_out_degrees[i], in_out_degrees[i+1]) for i in range(len(in_out_degrees) - 1)],\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.heads = nn.ModuleList([nn.Linear(in_out_degrees[-1], 1) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return [torch.sigmoid(head(x)) for head in self.heads]\n",
    "\n",
    "class MultiHeadComplexNet(nn.Module):\n",
    "    def __init__(self, in_out_degrees, num_heads, head_hidden_layers=2, head_hidden_dim=64):\n",
    "        super(MultiHeadComplexNet, self).__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            *[nn.Linear(in_out_degrees[i], in_out_degrees[i+1]) for i in range(len(in_out_degrees) - 1)],\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # More complex heads with multiple layers\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_out_degrees[-1], head_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                *[nn.Sequential(nn.Linear(head_hidden_dim, head_hidden_dim), nn.ReLU()) for _ in range(head_hidden_layers)],\n",
    "                nn.Linear(head_hidden_dim, 1)\n",
    "            ) \n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return [torch.sigmoid(head(x)) for head in self.heads]\n",
    "\n",
    "def kfoldcv(grouped_tweets, grouped_targets, target_list=[], \n",
    "            model_link='https://www.kaggle.com/models/google/nnlm/tensorFlow2/en-dim128/1',\n",
    "            train_batch_size=32, val_batch_size=32, weight_by_class=True, test_split=0.2, num_epochs = 3000):\n",
    "\n",
    "    saved_models = []\n",
    "    target_list = get_target_list(target_list)\n",
    "    tweet_dict = grouped_tweets_to_dict(grouped_tweets)\n",
    "    target_dict = grouped_targets_to_dict(grouped_targets, target_list)\n",
    "    intersecting_zips = get_intersection_keys(tweet_dict, target_dict)\n",
    "    paired_data = pair_data_together(intersecting_zips, tweet_dict, target_dict)\n",
    "    class_weights = get_class_weights(paired_data) if weight_by_class else np.zeros(len(target_list))\n",
    "    \n",
    "    X, y = paired_data_to_Xy(paired_data, hub_layer)\n",
    "\n",
    "    print(X.shape, y.shape)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    target_f1_scores = {target: [] for target in target_list}\n",
    "    target_accuracies = {target: [] for target in target_list}\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "        print(f\"Training fold {fold + 1}...\")\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        train_dataset = CustomDataset(X_train, y_train)\n",
    "        test_dataset = CustomDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(test_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "        \n",
    "        model = MultiHeadComplexNet([128, 256, 64, 32], num_heads=11).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        criterion = [nn.BCELoss() for _ in range(11)]\n",
    "        \n",
    "        best_model, history, history_per_target = loop(model, optimizer, criterion, n_epochs=num_epochs, \n",
    "                                                        train_dataloader=train_loader, val_dataloader=val_loader,\n",
    "                                                        model_name=f\"model_fold_{fold + 1}.pth\", device=device)\n",
    "        \n",
    "        saved_models.append(best_model)\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'f1_score': np.mean(history['f1']),\n",
    "            'accuracy': np.mean(history['acc']),\n",
    "            'history': history,\n",
    "            'history_per_target': history_per_target\n",
    "        })\n",
    "\n",
    "        for i, target in enumerate(target_list):\n",
    "            target_f1 = np.mean(history_per_target[i]['f1'])\n",
    "            target_acc = np.mean(history_per_target[i]['acc'])\n",
    "            target_f1_scores[target].append(target_f1)\n",
    "            target_accuracies[target].append(target_acc)\n",
    "\n",
    "    combined_results = {'fold': range(1, 6)}\n",
    "    for target in target_list:\n",
    "        combined_results[f'{target}_f1'] = target_f1_scores[target]\n",
    "        combined_results[f'{target}_accuracy'] = target_accuracies[target]\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_results)\n",
    "    print(combined_df.set_index('fold'))\n",
    "\n",
    "    # Compute average F1-score and accuracy for each target\n",
    "    avg_target_f1_scores = {target: np.mean(scores) for target, scores in target_f1_scores.items()}\n",
    "    avg_target_accuracies = {target: np.mean(scores) for target, scores in target_accuracies.items()}\n",
    "    \n",
    "    # Create a dictionary for the average row\n",
    "    avg_row = {'fold': 'Average'}\n",
    "    for target in target_list:\n",
    "        avg_row[f'{target}_f1'] = avg_target_f1_scores[target]\n",
    "        avg_row[f'{target}_accuracy'] = avg_target_accuracies[target]\n",
    "    \n",
    "    # Append the average row using pd.concat()\n",
    "    combined_df = pd.concat([combined_df, pd.DataFrame([avg_row])], ignore_index=True)\n",
    "    \n",
    "    # Print the final DataFrame with the added average row\n",
    "    print(combined_df.set_index('fold'))\n",
    "\n",
    "    # Define the file path\n",
    "    excel_file_path = os.path.join(save_path,\"kfold_results.xlsx\")\n",
    "    \n",
    "    # Save the DataFrame as an Excel file\n",
    "    combined_df.to_excel(excel_file_path, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {excel_file_path}\")\n",
    "\n",
    "    avg_f1 = np.mean([result['f1_score'] for result in fold_results])\n",
    "    avg_acc = np.mean([result['accuracy'] for result in fold_results])\n",
    "    print(f\"\\nAverage F1 Score: {avg_f1:.4f}, Average Accuracy: {avg_acc:.4f}\")\n",
    "    \n",
    "    return fold_results, saved_models\n",
    "\n",
    "def loop(model, optimizer, criterion, n_epochs, train_dataloader, val_dataloader, \n",
    "         model_name='model_state_bi.pth', device='mps', previous_loss_scale=1.05, epoch_percentage=0.1):\n",
    "    \n",
    "    history = {'train': [], 'test': [], 'f1': [], 'acc': []}\n",
    "    history_per_target = {i: {'f1': [], 'acc': []} for i in range(len(train_dataloader.dataset.labels[0]))}\n",
    "    previous_model = copy.deepcopy(model)\n",
    "    \n",
    "    with tqdm(range(n_epochs), desc=\"Training Progress\", unit=\"epoch\") as epoch_bar:\n",
    "        for epoch in epoch_bar:\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            num_batches = len(train_dataloader)\n",
    "            \n",
    "            for batch_X, batch_y in train_dataloader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "\n",
    "                # print(f\"Input shape: {batch_X.shape}\")\n",
    "                # for i in range(len(outputs)):\n",
    "                #     print(f\"Output shape: {outputs[i].shape}, Target shape: {batch_y[:, i].shape}\")\n",
    "                \n",
    "                # Compute total loss across all heads\n",
    "                loss = sum([criterion[i](outputs[i].squeeze(), batch_y[:, i]) for i in range(len(outputs))])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / num_batches\n",
    "            history['train'].append(avg_train_loss)\n",
    "\n",
    "            # Validation Phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            all_f1, all_acc = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_dataloader:\n",
    "                    batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                    test_outputs = model(batch_X)\n",
    "                    \n",
    "                    batch_loss = sum([criterion[i](test_outputs[i].squeeze(), batch_y[:, i]) for i in range(len(test_outputs))])\n",
    "                    val_loss += batch_loss.item()\n",
    "                    \n",
    "                    for i in range(batch_y.shape[1]):\n",
    "                        y_true = batch_y.cpu().numpy()[:, i]\n",
    "                        y_pred = (test_outputs[i].cpu().numpy() > 0.5).astype(int)\n",
    "                        \n",
    "                        f1 = f1_score(y_true, y_pred, average='binary', zero_division=0.0)\n",
    "                        acc = accuracy_score(y_true, y_pred)\n",
    "                        \n",
    "                        history_per_target[i]['f1'].append(f1)\n",
    "                        history_per_target[i]['acc'].append(acc)\n",
    "                        \n",
    "                        all_f1.append(f1)\n",
    "                        all_acc.append(acc)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            history['test'].append(avg_val_loss)\n",
    "            history['f1'].append(np.mean(all_f1))\n",
    "            history['acc'].append(np.mean(all_acc))\n",
    "\n",
    "            # Update tqdm with key metrics\n",
    "            epoch_bar.set_postfix(train_loss=avg_train_loss, val_loss=avg_val_loss, f1=np.mean(all_f1), acc=np.mean(all_acc))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, model_name))\n",
    "    return previous_model, history, history_per_target\n",
    "\n",
    "size_threshold = 80\n",
    "\n",
    "tweets_harvey2 = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/tweets/harvey_corrected.csv')\n",
    "tweets_harvey2.rename(columns={'zipcode': 'zip_code'}, inplace=True)\n",
    "\n",
    "bboxes_useful = tweets_harvey2.place_bbox.apply(lambda x: [[float(i.strip('()[]')) for i in x.split(', ')][i] for i in [1,0,3,2]])\n",
    "bbu_areas = bboxes_useful.apply(lambda x: get_box_area(*x))\n",
    "tweets_harvey = tweets_harvey2.loc[((tweets_harvey2.geo.apply(lambda x: 'Point' in str(x))) | (bbu_areas < size_threshold)), :]\n",
    "tweets_harvey.loc[:, 'zip_code'] = tweets_harvey['zip_code'].apply(int)\n",
    "tweet_grouped_harvey = tweets_harvey.groupby('zip_code')\n",
    "\n",
    "targets_harvey = pd.read_csv('D:/TAMIDS/Disaster_Impact_Estimation/targets/disaster_4332.csv')\n",
    "target_grouped_harvey = targets_harvey.groupby('damagedZipCode')\n",
    "\n",
    "X_dict_harvey = tweet_data_to_X_dict(grouped_tweets_to_dict(tweet_grouped_harvey), hub_layer)\n",
    "\n",
    "# After running the kfoldcv function:\n",
    "fold_results, saved_models = kfoldcv(tweet_grouped_harvey, target_grouped_harvey, train_batch_size = 512, val_batch_size=512, test_split=0.2, num_epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9394e2ab-6244-4e77-b8f8-503d6d933970",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_predictions = {}  # Dictionary to store predictions for each zipcode\n",
    "\n",
    "for zipcode, X in X_dict_harvey.items():\n",
    "    fold_preds = []  # Store predictions from each model\n",
    "\n",
    "    for model in saved_models:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            output_list = model(X_tensor)  # Output is a list of tensors (one per head)\n",
    "            \n",
    "            # Convert each tensor in the list to numpy format\n",
    "            pred = [out.cpu().numpy().astype(float) for out in output_list]\n",
    "            \n",
    "            fold_preds.append(pred)\n",
    "\n",
    "    # Convert fold_preds into a numpy array of shape (num_folds, num_heads)\n",
    "    fold_preds = np.array(fold_preds)  # Shape: (num_folds, num_heads)\n",
    "    \n",
    "    # Averaging predictions across the 5 folds for each head\n",
    "    final_pred = np.mean(fold_preds, axis=0).astype(float).squeeze()\n",
    "    \n",
    "    zipcode_predictions[zipcode] = final_pred  # Store final predictions\n",
    "\n",
    "# Create DataFrame\n",
    "result_df = pd.DataFrame.from_dict(zipcode_predictions, orient='index', columns=[\n",
    "    'homeOwnersInsurance', 'floodInsurance', 'destroyed', 'floodDamage', 'roofDamage', \n",
    "    'tsaEligible', 'tsaCheckedIn', 'rentalAssistanceEligible', 'repairAssistanceEligible', \n",
    "    'replacementAssistanceEligible', 'personalPropertyEligible'\n",
    "])\n",
    "\n",
    "# Reset index to have zipcode as a column\n",
    "result_df.reset_index(inplace=True)\n",
    "result_df.rename(columns={'index': 'zipcode'}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "result_df.to_csv(os.path.join(save_path,'zipcode_predictions_harvey_complex_multihead.csv'), index=False)\n",
    "print(\"Predictions saved to zipcode_predictions_harvey_complex_multihead.csv\")\n",
    "\n",
    "# Plotting the loss and F1 score curves\n",
    "for fold_result in fold_results:\n",
    "    history = fold_result['history']\n",
    "    history_per_target = fold_result['history_per_target']\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train'], label='Train Loss')\n",
    "    plt.plot(history['test'], label='Val Loss')\n",
    "    plt.title(f'Fold {fold_result[\"fold\"]} - Loss vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot F1 score per target\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(len(history_per_target)):\n",
    "        plt.plot(history_per_target[i]['f1'], label=f'Target {i}')\n",
    "    plt.title(f'Fold {fold_result[\"fold\"]} - F1 Score vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c31d37-fe75-4dc2-833e-388393f0f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = os.path.join(save_path,\"zipcode_predictions_harvey_complex_multihead.csv\")  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Calculate and print min and max for each column\n",
    "for column in df.select_dtypes(include=['number']).columns:\n",
    "    print(f\"{column}: Min = {df[column].min()}, Max = {df[column].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44cb8d-5293-4cb1-9e8e-19336b023122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
